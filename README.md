# Harshness Classifier Using Multiple CNNs
This project is based on the idea of classifying spectrogram images of music clips into two categories, clean, and harsh. The goal was to build a CNN model to accurately classify and identify harshness in audio. We felt that since CNNs are primarily used for image analysis, processing audio into images would be the perfect way to implement them! After the first CNN implementation, we implemented other CNNs to compare the performance of each of them. The main idea of this came from personal experience of music production. We wanted to combine our love for art with machine learning.

The original dataset was going to come from personal playlists using the Spotify API. However, after processing songs individually, it got extremely tedious and took way too much time. When stumbling upon the GTZAN Dataset on Kaggle, we discovered it was mostly used for training music genre classification. It had 1000 WAV files to use, and considering that we will eventually double that, we felt that the amount of data we had would be sufficient enough for this project.

After getting all of the audio clips, we needed a way to process them into images. A spectrogram, in simple terms, is a visual representation of audio signals. The horizontal axis represents time, and the vertical axis represents frequency. To generate these images, we used a library called librosa to process the spectrograms for each individual track. We originally used the documentation to implement this but ran into an error with 1 specific file that would not process. This ended up only giving us ~500 of the files that we needed. We asked ChatGPT to explain the error given our code and it generated code for error handling so although this one file won’t process, skip it, and process the rest. Now that we have our 999 files, we need to find a way to differentiate clean audio and harsh audio. We eventually found a GitHub repository called pydub, which in its own terms, “Pydub lets you do stuff to audio in a way that isn't stupid. ”. Using this library, I was able to manipulate the clean audio to simulate harshness by adding filtering, and slight distortion. The pydub repository has great documentation so writing that script wasn’t difficult. Finally, after we process our harsh audio, we then process them into spectrograms to get our 1998 files of data. Although we would like more data to better train these models, we had to work with what we had.

The original CNN is not included in this project considering that we eventually implemented more later on. The original CNN followed the exact guidelines from Homework 5 and it worked decently well. The results for the later implementations are in the final section of the colab file.
